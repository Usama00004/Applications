

\documentclass{resume} % Verwenden Sie den benutzerdefinierten resume.cls-Stil

\usepackage[left=0.4 in,top=0.2in,right=0.4 in,bottom=0.3in]{geometry} % Seitenränder
\newcommand{\tab}[1]{\hspace{.2667\textwidth}\rlap{#1}} 
\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\name{Usama Tahir} % Fügen Sie hier Ihren vollständigen Namen hinzu
\address{+49 17684973934 \\ Deutschland}
\address{\href{mailto:usamatahir00004@gmail.com}{E-Mail} \\ \href{http://www.linkedin.com/in/usamatahir-00004}{LinkedIn} \\ 
\href{https://github.com/Usama00004}{GitHub}} 

\begin{document}

%----------------------------------------------------------------------------------------
%	ZIEL
%----------------------------------------------------------------------------------------

\begin{rSection}{ZIEL}

Detailorientierter Softwareingenieur mit Erfahrung in Datenanalyse, Web-Scraping und Visualisierung. Kompetent in Python, Power BI und SQL, suche eine Teilzeitstelle als Data Analyst, um datengetriebene Entscheidungen zu fördern.

\end{rSection}

%----------------------------------------------------------------------------------------
%	AUSBILDUNG
%----------------------------------------------------------------------------------------

\begin{rSection}{AUSBILDUNG}

{\bf Master in Global Software Development}, Hochschule Fulda \hfill {März 2026}\\
{\bf Bachelor in Informatik}, COMSATS Lahore \hfill {2016 - 2020}

\end{rSection}

%----------------------------------------------------------------------------------------
% FÄHIGKEITEN
%----------------------------------------------------------------------------------------

\begin{rSection}{FÄHIGKEITEN}

\textbf{Technische Fähigkeiten:} Python, Django, Selenium, Scrapy, Beautiful Soup, Pandas, NumPy, SQL, PowerBI, Git, Excel \\
\textbf{Soft Skills:} Kommunikation, Problemlösung, Anpassungsfähigkeit, Zeitmanagement \\
\textbf{Sprachen:} Deutsch (B1 CEFR), Englisch (C1 CEFR), Urdu, Punjabi, Hindi 

\end{rSection}

%----------------------------------------------------------------------------------------
%	BERUFSERFAHRUNG
%----------------------------------------------------------------------------------------

\begin{rSection}{BERUFSERFAHRUNG}

\textbf{Junior Data Analyst} \hfill Jan 2021 - Juli 2022\\
Systems Limited \hfill \textit{Lahore, Pakistan}
\begin{itemize}
    \item Automatisierte ETL-Prozesse und Datenpipelines mit Python (Beautiful Soup, Selenium).
    \item Implementierte Echtzeit-Datenströme mit Kafka für effiziente Datenaufnahme und -verarbeitung.
    \item Verwendete SQL zur Datentransformation und -integration in Pipelines.
    \item Nutzte Power BI für Datenvisualisierung und Berichterstellung, um umsetzbare Erkenntnisse zu ermöglichen.
    \item Steigerte die Datenverfügbarkeit um 30\% und reduzierte die Verarbeitungszeit um 20\%, wodurch die Entscheidungsfindung verbessert wurde.
\end{itemize}

\end{rSection}

%----------------------------------------------------------------------------------------
%	PROJEKTE
%----------------------------------------------------------------------------------------
\begin{rSection}{PROJEKTE}

\item \textbf{Datenanalyse-Dashboard:} Entwickelte ein Power BI-Dashboard zur Visualisierung von Trends aus bereinigten Daten (Python, Pandas). Sicherte Datenintegrität mit Vorverarbeitungspipelines, bereitgestellt in der Cloud für Echtzeitanalysen, und reduzierte die Datenverarbeitungszeit durch Fehlerbehandlung um 20%. 
(verfügbar auf \href{https://github.com/Usama00004/SalesReport}{GitHub}).

\item \textbf{Echtzeit-Datenströme:} Entwickelte und implementierte eine Pipeline für Echtzeit-Datenströme, um hochfrequente Daten mit Apache Kafka und Apache Flink zu verarbeiten und zu analysieren. Daten wurden in Echtzeit aufgenommen, transformiert und in PostgreSQL für die weitere Analyse gespeichert, was latenzarme Einblicke und effiziente Datenverarbeitung für groß angelegte Systeme ermöglichte 
(verfügbar auf \href{https://github.com/Usama00004/StreamAnalysis}{GitHub}).

\item \textbf{Sentiment-Analyse:} Entwickelte eine Datenpipeline, die Web-Scraping und Sentiment-Analyse integriert. Automatisierte die Datenerfassung mit BeautifulSoup und Selenium, wodurch der manuelle Aufwand um 40\% reduziert wurde. Bereinigte und validierte Daten mit Pandas, speicherte sie in SQL und setzte Scikit-learn-Modelle für Echtzeitanalysen ein, wodurch die Verarbeitungseffizienz um 25\% verbessert wurde 
(verfügbar auf \href{https://github.com/Usama00004/Twitter-Sentiment-Analysis}{GitHub}).

\end{rSection}

%----------------------------------------------------------------------------------------
%	VERÖFFENTLICHUNGEN
%----------------------------------------------------------------------------------------

\begin{rSection}{VERÖFFENTLICHUNGEN}
\item \textbf{Hotspot-Aware Workload Scheduling and Server Placement for Heterogeneous Cloud Data Centers:} Erforschung und Entwicklung eines neuartigen Ansatzes zur hotspotbewussten Arbeitslastplanung und Serverplatzierung in heterogenen Cloud-Rechenzentren. Veröffentlicht im MDPI Energies Journal, schlägt das Papier innovative Algorithmen und Methoden zur Optimierung der Ressourcennutzung und Minimierung des Energieverbrauchs in Cloud-Umgebungen vor 
(verfügbar auf \href{https://www.mdpi.com/1996-1073/15/7/2541}{MDPI Energies}).

\end{rSection}

\end{document}
